{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_for_protein_sequences.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dauparas/tensorflow2_ipynb/blob/master/BERT_for_protein_sequences.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mj-HPCtV-xvU",
        "colab_type": "code",
        "outputId": "f00d0ea0-fb40-4991-a19b-b67a525307ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#@title Import dependencies { display-mode: \"form\" }\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "# Install TensorFlow\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.python import tf2\n",
        "if not tf2.enabled():\n",
        "  import tensorflow.compat.v2 as tf\n",
        "  tf.enable_v2_behavior()\n",
        "  assert tf2.enabled()\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "\n",
        "tfk = tf.keras\n",
        "tfkl = tf.keras.layers\n",
        "tfpl = tfp.layers\n",
        "tfd = tfp.distributions\n",
        "from keras.layers import Input, Embedding, LSTM, Dense\n",
        "\n",
        "import pandas as pd\n",
        "from scipy.sparse import csr_matrix\n",
        "import umap\n",
        "from sklearn.metrics.cluster import adjusted_rand_score\n",
        "\n",
        "import h5py\n",
        "#Step 1: import dependencies\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "from __future__ import division\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.datasets import make_blobs\n",
        "import seaborn as sns; sns.set()\n",
        "import keras.backend as K\n",
        "import os\n",
        "import random\n",
        "\n",
        "%matplotlib inline\n",
        "plt.style.use('dark_background')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d7Fe2q--KMU",
        "colab_type": "text"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijbQXxzlYKAk",
        "colab_type": "text"
      },
      "source": [
        "## Convert FASTA to MSA np.array()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wU8Gzh1FQEZl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parse_fasta(filename):\n",
        "  '''function to parse fasta file'''\n",
        "  header = []\n",
        "  sequence = []\n",
        "  lines = open(filename, \"r\")\n",
        "  for line in lines:\n",
        "    line = line.rstrip()\n",
        "    if line[0] == \">\":\n",
        "      header.append(line[1:])\n",
        "      sequence.append([])\n",
        "    else:\n",
        "      sequence[-1].append(line)\n",
        "  lines.close()\n",
        "  sequence = [''.join(seq) for seq in sequence]\n",
        "  return np.array(header), np.array(sequence)\n",
        "  \n",
        "def mk_msa(seqs):\n",
        "  '''one hot encode msa'''\n",
        "  \n",
        "  ################\n",
        "  alphabet = \"ARNDCQEGHILKMFPSTWYV-\"\n",
        "  states = len(alphabet)\n",
        "  a2n = {}\n",
        "  for a,n in zip(alphabet,range(states)):\n",
        "    a2n[a] = n\n",
        "\n",
        "  def aa2num(aa):\n",
        "    '''convert aa into num'''\n",
        "    if aa in a2n: return a2n[aa]\n",
        "    else: return a2n['-']\n",
        "  ################\n",
        "  \n",
        "  msa = []\n",
        "  for seq in seqs:\n",
        "    msa.append([aa2num(aa) for aa in seq])\n",
        "  msa_ori = np.array(msa)\n",
        "  return msa_ori, tf.keras.utils.to_categorical(msa_ori,states)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nn5oow4zP9ht",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -q -nc https://gremlin2.bakerlab.org/db/PDB_EXP/fasta/1BXYA.fas"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipoWnWOCQryY",
        "colab_type": "code",
        "outputId": "32f0860d-699e-4012-8d0f-c0a4d3334ee8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "names,seqs = parse_fasta(\"1BXYA.fas\")\n",
        "msa_ori, msa = mk_msa(seqs)\n",
        "\n",
        "print(msa_ori.shape)\n",
        "print(msa.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2025, 60)\n",
            "(2025, 60, 21)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InL-iW_wsuz7",
        "colab_type": "code",
        "outputId": "0033939d-6c60-4889-d324-ccde3c14af05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(\"ARNDCQEGHILKMFPSTWYV-\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKeh3GCv6k5O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def new_mask(msa_ori, p=0.95):\n",
        "  mask_inpt = np.random.binomial(1, p, size=msa_ori.shape)\n",
        "  indx = np.argwhere(mask_inpt == 0)\n",
        "  rand_indx = indx[np.random.choice(indx.shape[0], indx.shape[0], replace=False),:]\n",
        "  masked_msa_inpt = np.copy(msa_ori)\n",
        "  for i in range(indx.shape[0]):\n",
        "    if i < np.int(indx.shape[0]*0.8):\n",
        "      i1, i2 = rand_indx[i,:]\n",
        "      masked_msa_inpt[i1, i2] = 21\n",
        "    elif np.int(indx.shape[0]*0.8) <= i < np.int(indx.shape[0]*0.9):\n",
        "      i1, i2 = rand_indx[i,:]\n",
        "      masked_msa_inpt[i1, i2] = np.random.randint(21)\n",
        "      \n",
        "  return msa_ori, masked_msa_inpt, mask_inpt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UEMMSActOev",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MSA, MSA_MASKED, MASK = new_mask(msa_ori, p=0.95)\n",
        "batch_size = 32\n",
        "train_dataset = (msa_ori, MASK.astype(np.float32), MSA_MASKED)\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(train_dataset)\n",
        "train_dataset = train_dataset.shuffle(buffer_size=10000)\n",
        "train_dataset = train_dataset.batch(batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Envz7T5RxiY8",
        "colab_type": "code",
        "outputId": "6725b56b-cc82-4e8b-a2b8-a99d29eab096",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_dataset"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((None, 60), (None, 60), (None, 60)), types: (tf.int64, tf.float32, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eaxd501k-Wlh",
        "colab_type": "text"
      },
      "source": [
        "# Transformer model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6agELVBx0p9i",
        "colab_type": "text"
      },
      "source": [
        "Based on:\n",
        "https://colab.research.google.com/github/tensorflow/docs/blob/r2.0rc/site/en/r2/tutorials/text/transformer.ipynb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOu43hA1DjG1",
        "colab_type": "text"
      },
      "source": [
        "# Attention layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2SK2-JdPDWF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates\n",
        "\n",
        "def positional_encoding(position, d_model): #position = length of seq\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "  \n",
        "  # apply sin to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "  \n",
        "  # apply cos to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    \n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "    \n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3r9YdZH1HAn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask=None):\n",
        "  \"\"\"Calculate the attention weights.\n",
        "  q, k, v must have matching leading dimensions.\n",
        "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "  The mask has different shapes depending on its type(padding or look ahead) \n",
        "  but it must be broadcastable for addition.\n",
        "  \n",
        "  Args:\n",
        "    q: query shape == (..., seq_len_q, depth)\n",
        "    k: key shape == (..., seq_len_k, depth)\n",
        "    v: value shape == (..., seq_len_v, depth_v)\n",
        "    mask: Float tensor with shape broadcastable \n",
        "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "    \n",
        "  Returns:\n",
        "    output, attention_weights\n",
        "  \"\"\"\n",
        "\n",
        "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "  \n",
        "  # scale matmul_qk\n",
        "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "  # add the mask to the scaled tensor.\n",
        "  if mask is not None:\n",
        "    scaled_attention_logits += (mask * -1e9)  \n",
        "\n",
        "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "  # add up to 1.\n",
        "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "  return output, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vAfEVZI1gaL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "    \n",
        "    assert d_model % self.num_heads == 0\n",
        "    \n",
        "    self.depth = d_model // self.num_heads\n",
        "    \n",
        "    self.wq = tf.keras.layers.Dense(d_model)\n",
        "    self.wk = tf.keras.layers.Dense(d_model)\n",
        "    self.wv = tf.keras.layers.Dense(d_model)\n",
        "    \n",
        "    self.dense = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "  def split_heads(self, x, batch_size):\n",
        "    \"\"\"Split the last dimension into (num_heads, depth).\n",
        "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "    \"\"\"\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "  def call(self, v, k, q, mask):\n",
        "    batch_size = tf.shape(q)[0]\n",
        "    \n",
        "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "    \n",
        "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "    \n",
        "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "        q, k, v, mask)\n",
        "    \n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "    concat_attention = tf.reshape(scaled_attention, \n",
        "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "        \n",
        "    return output, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMfqnJIE116E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "  ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mb5KZrgv15ed",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    \n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    attn_output, attention_weights = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "    attn_output = self.dropout1(attn_output, training=training)\n",
        "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "    \n",
        "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "    \n",
        "    return out2, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPTNTXFL2MB6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
        "               rate=0.1):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "    \n",
        "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(input_vocab_size, self.d_model)\n",
        "    \n",
        "    \n",
        "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
        "                       for _ in range(num_layers)]\n",
        "  \n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    \n",
        "    # adding embedding and position encoding.\n",
        "    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "    #x = tf.one_hot(x, depth=self.d_model, axis=-1)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "    \n",
        "    for i in range(self.num_layers):\n",
        "      x, attn_weights = self.enc_layers[i](x, training, mask)\n",
        "      \n",
        "    \n",
        "    return x, attn_weights  # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7Xjhwm1pMJ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(3e-4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qN1d0t8rosCk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_encoder = Encoder(6, 100, 10, 100*4, 60)\n",
        "final_layer = tf.keras.layers.Dense(22, name='fO')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GrlwQvaocFa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Notice the use of `tf.function`\n",
        "# This annotation causes the function to be \"compiled\".\n",
        "@tf.function\n",
        "def train_step(MSA, MASK, MSA_MASKED):\n",
        "    with tf.GradientTape() as tape:\n",
        "      Z, attn_weights= sample_encoder(MSA_MASKED, training=True, mask=None)\n",
        "      O = final_layer(Z, training=True)\n",
        "      H = tf.nn.softmax_cross_entropy_with_logits(labels = tf.one_hot(MSA, 22, axis=-1), logits=O,  axis=-1)\n",
        "      ECE = tf.reduce_mean(tf.exp(H))\n",
        "      l1 = H*(1-MASK)\n",
        "      loss = tf.reduce_sum(l1)\n",
        "      correct = tf.equal(tf.argmax(tf.nn.softmax(O, axis=-1), -1), MSA)\n",
        "      correct = tf.cast(correct, tf.float32)\n",
        "      accuracy = tf.reduce_mean(correct)\n",
        "\n",
        "    gradients = tape.gradient(loss, sample_encoder.trainable_variables+final_layer.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, sample_encoder.trainable_variables+final_layer.trainable_variables))\n",
        "    return loss, accuracy, ECE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNibm_4vsRxw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "    \n",
        "    for MSA, MASK, MSA_MASKED in dataset:\n",
        "      loss, accuracy, ECE = train_step(MSA, MASK, MSA_MASKED)\n",
        "    print('Loss: {0:.3f}, Accuracy: {1:.3f}, ECE: {2:.3f}'.format(loss.numpy(), accuracy.numpy(), ECE.numpy()))\n",
        "  return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwHJ7Kp6sUGo",
        "colab_type": "code",
        "outputId": "3947e2be-2372-438e-ba08-aa887d8eb59d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "for i in range(100):\n",
        "\n",
        "  MSA, MSA_MASKED, MASK = new_mask(msa_ori, p=0.85)\n",
        "  batch_size = 32\n",
        "  train_dataset = (msa_ori, MASK.astype(np.float32), MSA_MASKED)\n",
        "  train_dataset = tf.data.Dataset.from_tensor_slices(train_dataset)\n",
        "  train_dataset = train_dataset.shuffle(buffer_size=10000)\n",
        "  train_dataset = train_dataset.batch(batch_size)\n",
        "  \n",
        "  train(train_dataset, 5)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 286.369, Accuracy: 0.106, ECE: 20.184\n",
            "Loss: 254.011, Accuracy: 0.102, ECE: 21.263\n",
            "Loss: 273.981, Accuracy: 0.100, ECE: 22.832\n",
            "Loss: 204.305, Accuracy: 0.083, ECE: 20.068\n",
            "Loss: 217.799, Accuracy: 0.124, ECE: 19.937\n",
            "Loss: 221.586, Accuracy: 0.102, ECE: 19.228\n",
            "Loss: 222.948, Accuracy: 0.104, ECE: 22.053\n",
            "Loss: 224.768, Accuracy: 0.087, ECE: 23.574\n",
            "Loss: 214.475, Accuracy: 0.106, ECE: 26.328\n",
            "Loss: 190.273, Accuracy: 0.120, ECE: 18.952\n",
            "Loss: 256.411, Accuracy: 0.106, ECE: 23.358\n",
            "Loss: 231.507, Accuracy: 0.107, ECE: 23.047\n",
            "Loss: 202.638, Accuracy: 0.109, ECE: 19.394\n",
            "Loss: 237.967, Accuracy: 0.100, ECE: 19.881\n",
            "Loss: 257.294, Accuracy: 0.133, ECE: 21.478\n",
            "Loss: 258.269, Accuracy: 0.100, ECE: 22.506\n",
            "Loss: 221.815, Accuracy: 0.094, ECE: 24.570\n",
            "Loss: 245.861, Accuracy: 0.109, ECE: 19.651\n",
            "Loss: 228.899, Accuracy: 0.113, ECE: 18.795\n",
            "Loss: 232.342, Accuracy: 0.122, ECE: 18.623\n",
            "Loss: 201.864, Accuracy: 0.120, ECE: 22.620\n",
            "Loss: 224.761, Accuracy: 0.106, ECE: 19.667\n",
            "Loss: 190.852, Accuracy: 0.117, ECE: 19.223\n",
            "Loss: 232.273, Accuracy: 0.124, ECE: 19.706\n",
            "Loss: 210.722, Accuracy: 0.107, ECE: 20.172\n",
            "Loss: 222.061, Accuracy: 0.126, ECE: 23.858\n",
            "Loss: 219.485, Accuracy: 0.122, ECE: 23.158\n",
            "Loss: 256.018, Accuracy: 0.102, ECE: 22.325\n",
            "Loss: 220.452, Accuracy: 0.117, ECE: 18.635\n",
            "Loss: 212.394, Accuracy: 0.106, ECE: 23.657\n",
            "Loss: 221.796, Accuracy: 0.159, ECE: 18.881\n",
            "Loss: 212.823, Accuracy: 0.231, ECE: 19.372\n",
            "Loss: 229.193, Accuracy: 0.267, ECE: 18.666\n",
            "Loss: 234.274, Accuracy: 0.309, ECE: 25.269\n",
            "Loss: 144.818, Accuracy: 0.383, ECE: 14.901\n",
            "Loss: 161.753, Accuracy: 0.485, ECE: 11.659\n",
            "Loss: 127.183, Accuracy: 0.644, ECE: 11.405\n",
            "Loss: 147.730, Accuracy: 0.689, ECE: 12.005\n",
            "Loss: 103.960, Accuracy: 0.722, ECE: 10.052\n",
            "Loss: 146.480, Accuracy: 0.722, ECE: 11.993\n",
            "Loss: 132.371, Accuracy: 0.767, ECE: 12.047\n",
            "Loss: 112.423, Accuracy: 0.789, ECE: 6.135\n",
            "Loss: 84.073, Accuracy: 0.813, ECE: 8.749\n",
            "Loss: 81.516, Accuracy: 0.809, ECE: 6.652\n",
            "Loss: 99.896, Accuracy: 0.806, ECE: 17.292\n",
            "Loss: 92.311, Accuracy: 0.813, ECE: 9.559\n",
            "Loss: 65.953, Accuracy: 0.850, ECE: 5.779\n",
            "Loss: 101.878, Accuracy: 0.802, ECE: 9.125\n",
            "Loss: 95.429, Accuracy: 0.844, ECE: 7.317\n",
            "Loss: 99.547, Accuracy: 0.848, ECE: 12.878\n",
            "Loss: 87.735, Accuracy: 0.837, ECE: 6.323\n",
            "Loss: 85.670, Accuracy: 0.856, ECE: 7.647\n",
            "Loss: 111.593, Accuracy: 0.826, ECE: 17.865\n",
            "Loss: 127.591, Accuracy: 0.806, ECE: 9.435\n",
            "Loss: 75.115, Accuracy: 0.911, ECE: 5.331\n",
            "Loss: 85.974, Accuracy: 0.854, ECE: 7.425\n",
            "Loss: 72.956, Accuracy: 0.883, ECE: 3.839\n",
            "Loss: 46.559, Accuracy: 0.887, ECE: 4.267\n",
            "Loss: 70.305, Accuracy: 0.893, ECE: 4.601\n",
            "Loss: 54.171, Accuracy: 0.859, ECE: 6.331\n",
            "Loss: 78.625, Accuracy: 0.844, ECE: 6.778\n",
            "Loss: 84.345, Accuracy: 0.867, ECE: 7.676\n",
            "Loss: 66.829, Accuracy: 0.861, ECE: 6.434\n",
            "Loss: 94.857, Accuracy: 0.841, ECE: 17.989\n",
            "Loss: 68.871, Accuracy: 0.874, ECE: 4.976\n",
            "Loss: 109.094, Accuracy: 0.835, ECE: 7.312\n",
            "Loss: 83.290, Accuracy: 0.863, ECE: 5.032\n",
            "Loss: 62.929, Accuracy: 0.869, ECE: 5.289\n",
            "Loss: 110.440, Accuracy: 0.828, ECE: 6.144\n",
            "Loss: 87.917, Accuracy: 0.841, ECE: 9.905\n",
            "Loss: 72.966, Accuracy: 0.880, ECE: 3.694\n",
            "Loss: 72.232, Accuracy: 0.863, ECE: 7.695\n",
            "Loss: 76.113, Accuracy: 0.883, ECE: 3.777\n",
            "Loss: 92.862, Accuracy: 0.856, ECE: 5.287\n",
            "Loss: 56.285, Accuracy: 0.869, ECE: 4.425\n",
            "Loss: 106.369, Accuracy: 0.848, ECE: 6.345\n",
            "Loss: 80.659, Accuracy: 0.885, ECE: 4.786\n",
            "Loss: 82.183, Accuracy: 0.844, ECE: 13.159\n",
            "Loss: 63.180, Accuracy: 0.896, ECE: 5.820\n",
            "Loss: 68.205, Accuracy: 0.893, ECE: 4.942\n",
            "Loss: 87.300, Accuracy: 0.861, ECE: 5.727\n",
            "Loss: 53.081, Accuracy: 0.885, ECE: 6.867\n",
            "Loss: 56.363, Accuracy: 0.878, ECE: 5.583\n",
            "Loss: 109.720, Accuracy: 0.859, ECE: 28.563\n",
            "Loss: 90.017, Accuracy: 0.865, ECE: 8.705\n",
            "Loss: 98.221, Accuracy: 0.863, ECE: 7.540\n",
            "Loss: 73.168, Accuracy: 0.885, ECE: 3.538\n",
            "Loss: 91.791, Accuracy: 0.870, ECE: 4.087\n",
            "Loss: 62.832, Accuracy: 0.869, ECE: 5.536\n",
            "Loss: 72.286, Accuracy: 0.893, ECE: 8.761\n",
            "Loss: 65.643, Accuracy: 0.894, ECE: 6.839\n",
            "Loss: 65.084, Accuracy: 0.872, ECE: 36.337\n",
            "Loss: 71.280, Accuracy: 0.861, ECE: 8.391\n",
            "Loss: 62.072, Accuracy: 0.893, ECE: 6.125\n",
            "Loss: 77.766, Accuracy: 0.865, ECE: 11.055\n",
            "Loss: 64.771, Accuracy: 0.883, ECE: 16.773\n",
            "Loss: 86.783, Accuracy: 0.854, ECE: 5.985\n",
            "Loss: 79.350, Accuracy: 0.861, ECE: 8.475\n",
            "Loss: 75.982, Accuracy: 0.876, ECE: 4.142\n",
            "Loss: 102.135, Accuracy: 0.881, ECE: 9.599\n",
            "Loss: 100.159, Accuracy: 0.831, ECE: 11.756\n",
            "Loss: 72.779, Accuracy: 0.889, ECE: 5.687\n",
            "Loss: 75.568, Accuracy: 0.893, ECE: 3.875\n",
            "Loss: 66.568, Accuracy: 0.865, ECE: 4.967\n",
            "Loss: 80.082, Accuracy: 0.863, ECE: 6.806\n",
            "Loss: 91.642, Accuracy: 0.843, ECE: 12.551\n",
            "Loss: 67.099, Accuracy: 0.872, ECE: 4.161\n",
            "Loss: 71.405, Accuracy: 0.896, ECE: 4.874\n",
            "Loss: 82.123, Accuracy: 0.872, ECE: 4.320\n",
            "Loss: 58.272, Accuracy: 0.874, ECE: 5.240\n",
            "Loss: 71.596, Accuracy: 0.898, ECE: 5.557\n",
            "Loss: 68.166, Accuracy: 0.870, ECE: 14.067\n",
            "Loss: 54.491, Accuracy: 0.909, ECE: 2.916\n",
            "Loss: 79.284, Accuracy: 0.857, ECE: 19.079\n",
            "Loss: 65.292, Accuracy: 0.861, ECE: 4.706\n",
            "Loss: 70.722, Accuracy: 0.861, ECE: 5.156\n",
            "Loss: 45.975, Accuracy: 0.902, ECE: 4.104\n",
            "Loss: 64.584, Accuracy: 0.878, ECE: 8.339\n",
            "Loss: 86.655, Accuracy: 0.870, ECE: 7.025\n",
            "Loss: 59.597, Accuracy: 0.904, ECE: 3.258\n",
            "Loss: 69.889, Accuracy: 0.883, ECE: 11.998\n",
            "Loss: 69.675, Accuracy: 0.872, ECE: 4.497\n",
            "Loss: 74.755, Accuracy: 0.880, ECE: 18.118\n",
            "Loss: 93.168, Accuracy: 0.887, ECE: 8.045\n",
            "Loss: 74.176, Accuracy: 0.887, ECE: 12.552\n",
            "Loss: 71.148, Accuracy: 0.863, ECE: 8.227\n",
            "Loss: 53.475, Accuracy: 0.900, ECE: 7.101\n",
            "Loss: 56.192, Accuracy: 0.885, ECE: 34.976\n",
            "Loss: 78.409, Accuracy: 0.867, ECE: 4.744\n",
            "Loss: 48.909, Accuracy: 0.900, ECE: 4.460\n",
            "Loss: 100.776, Accuracy: 0.850, ECE: 8.314\n",
            "Loss: 80.811, Accuracy: 0.881, ECE: 13.129\n",
            "Loss: 78.060, Accuracy: 0.874, ECE: 11.531\n",
            "Loss: 63.472, Accuracy: 0.896, ECE: 18.693\n",
            "Loss: 59.376, Accuracy: 0.896, ECE: 3.366\n",
            "Loss: 60.309, Accuracy: 0.907, ECE: 2.822\n",
            "Loss: 55.633, Accuracy: 0.880, ECE: 5.002\n",
            "Loss: 42.339, Accuracy: 0.913, ECE: 3.309\n",
            "Loss: 41.594, Accuracy: 0.893, ECE: 7.980\n",
            "Loss: 53.155, Accuracy: 0.900, ECE: 15.291\n",
            "Loss: 64.428, Accuracy: 0.909, ECE: 5.261\n",
            "Loss: 68.534, Accuracy: 0.880, ECE: 3.669\n",
            "Loss: 52.334, Accuracy: 0.904, ECE: 4.103\n",
            "Loss: 73.799, Accuracy: 0.872, ECE: 6.499\n",
            "Loss: 48.764, Accuracy: 0.870, ECE: 4.259\n",
            "Loss: 62.423, Accuracy: 0.893, ECE: 4.868\n",
            "Loss: 55.496, Accuracy: 0.926, ECE: 2.177\n",
            "Loss: 49.884, Accuracy: 0.909, ECE: 14.242\n",
            "Loss: 66.111, Accuracy: 0.906, ECE: 3.222\n",
            "Loss: 66.429, Accuracy: 0.907, ECE: 2.894\n",
            "Loss: 55.747, Accuracy: 0.898, ECE: 4.680\n",
            "Loss: 52.068, Accuracy: 0.911, ECE: 8.717\n",
            "Loss: 72.399, Accuracy: 0.889, ECE: 6.560\n",
            "Loss: 40.976, Accuracy: 0.920, ECE: 1.687\n",
            "Loss: 62.007, Accuracy: 0.894, ECE: 10.826\n",
            "Loss: 57.998, Accuracy: 0.902, ECE: 4.405\n",
            "Loss: 86.290, Accuracy: 0.874, ECE: 12.147\n",
            "Loss: 70.445, Accuracy: 0.893, ECE: 5.941\n",
            "Loss: 41.332, Accuracy: 0.919, ECE: 1.998\n",
            "Loss: 66.551, Accuracy: 0.889, ECE: 14.004\n",
            "Loss: 88.393, Accuracy: 0.865, ECE: 7.592\n",
            "Loss: 28.505, Accuracy: 0.933, ECE: 2.234\n",
            "Loss: 74.856, Accuracy: 0.900, ECE: 2.910\n",
            "Loss: 70.298, Accuracy: 0.906, ECE: 9.130\n",
            "Loss: 50.187, Accuracy: 0.894, ECE: 18.360\n",
            "Loss: 42.460, Accuracy: 0.904, ECE: 10.885\n",
            "Loss: 56.222, Accuracy: 0.893, ECE: 2.928\n",
            "Loss: 51.324, Accuracy: 0.894, ECE: 2.383\n",
            "Loss: 61.541, Accuracy: 0.869, ECE: 9.385\n",
            "Loss: 48.629, Accuracy: 0.894, ECE: 4.958\n",
            "Loss: 76.944, Accuracy: 0.876, ECE: 3.512\n",
            "Loss: 78.795, Accuracy: 0.885, ECE: 4.463\n",
            "Loss: 39.221, Accuracy: 0.933, ECE: 2.465\n",
            "Loss: 40.270, Accuracy: 0.919, ECE: 1.992\n",
            "Loss: 60.453, Accuracy: 0.887, ECE: 7.702\n",
            "Loss: 76.931, Accuracy: 0.854, ECE: 5.730\n",
            "Loss: 90.577, Accuracy: 0.881, ECE: 7.447\n",
            "Loss: 40.825, Accuracy: 0.933, ECE: 4.358\n",
            "Loss: 35.230, Accuracy: 0.924, ECE: 2.090\n",
            "Loss: 40.572, Accuracy: 0.920, ECE: 19.210\n",
            "Loss: 67.417, Accuracy: 0.893, ECE: 4.802\n",
            "Loss: 67.250, Accuracy: 0.891, ECE: 4.182\n",
            "Loss: 50.760, Accuracy: 0.878, ECE: 7.353\n",
            "Loss: 71.094, Accuracy: 0.878, ECE: 7.857\n",
            "Loss: 62.221, Accuracy: 0.883, ECE: 15.340\n",
            "Loss: 61.475, Accuracy: 0.906, ECE: 14.775\n",
            "Loss: 69.738, Accuracy: 0.880, ECE: 43.159\n",
            "Loss: 56.899, Accuracy: 0.898, ECE: 7.897\n",
            "Loss: 63.394, Accuracy: 0.887, ECE: 9.249\n",
            "Loss: 42.571, Accuracy: 0.922, ECE: 2.548\n",
            "Loss: 83.981, Accuracy: 0.896, ECE: 17.336\n",
            "Loss: 50.629, Accuracy: 0.869, ECE: 6.761\n",
            "Loss: 76.052, Accuracy: 0.874, ECE: 4.685\n",
            "Loss: 48.383, Accuracy: 0.902, ECE: 4.721\n",
            "Loss: 53.591, Accuracy: 0.913, ECE: 7.659\n",
            "Loss: 58.545, Accuracy: 0.898, ECE: 3.977\n",
            "Loss: 79.750, Accuracy: 0.887, ECE: 17.736\n",
            "Loss: 70.538, Accuracy: 0.896, ECE: 5.763\n",
            "Loss: 64.614, Accuracy: 0.902, ECE: 4.805\n",
            "Loss: 51.059, Accuracy: 0.907, ECE: 3.320\n",
            "Loss: 65.750, Accuracy: 0.881, ECE: 3.951\n",
            "Loss: 65.138, Accuracy: 0.898, ECE: 8.134\n",
            "Loss: 61.376, Accuracy: 0.933, ECE: 23.034\n",
            "Loss: 54.890, Accuracy: 0.913, ECE: 5.361\n",
            "Loss: 35.892, Accuracy: 0.926, ECE: 2.223\n",
            "Loss: 54.860, Accuracy: 0.896, ECE: 6.845\n",
            "Loss: 59.138, Accuracy: 0.909, ECE: 2.668\n",
            "Loss: 66.258, Accuracy: 0.919, ECE: 107.058\n",
            "Loss: 59.484, Accuracy: 0.894, ECE: 4.075\n",
            "Loss: 65.729, Accuracy: 0.900, ECE: 28.486\n",
            "Loss: 71.252, Accuracy: 0.898, ECE: 14.736\n",
            "Loss: 33.365, Accuracy: 0.915, ECE: 3.113\n",
            "Loss: 55.484, Accuracy: 0.915, ECE: 2.882\n",
            "Loss: 62.336, Accuracy: 0.900, ECE: 5.724\n",
            "Loss: 51.942, Accuracy: 0.919, ECE: 3.236\n",
            "Loss: 68.484, Accuracy: 0.894, ECE: 21.145\n",
            "Loss: 57.257, Accuracy: 0.907, ECE: 24.605\n",
            "Loss: 51.432, Accuracy: 0.915, ECE: 3.912\n",
            "Loss: 45.012, Accuracy: 0.900, ECE: 4.741\n",
            "Loss: 71.906, Accuracy: 0.889, ECE: 3.745\n",
            "Loss: 47.227, Accuracy: 0.893, ECE: 4.454\n",
            "Loss: 69.393, Accuracy: 0.919, ECE: 2.818\n",
            "Loss: 82.540, Accuracy: 0.885, ECE: 4.287\n",
            "Loss: 67.875, Accuracy: 0.896, ECE: 3.462\n",
            "Loss: 46.410, Accuracy: 0.891, ECE: 7.498\n",
            "Loss: 52.355, Accuracy: 0.909, ECE: 3.146\n",
            "Loss: 41.718, Accuracy: 0.931, ECE: 3.200\n",
            "Loss: 71.825, Accuracy: 0.896, ECE: 46.808\n",
            "Loss: 58.072, Accuracy: 0.889, ECE: 8.818\n",
            "Loss: 67.307, Accuracy: 0.902, ECE: 2.642\n",
            "Loss: 61.502, Accuracy: 0.900, ECE: 2.389\n",
            "Loss: 64.567, Accuracy: 0.900, ECE: 3.896\n",
            "Loss: 79.658, Accuracy: 0.869, ECE: 11.375\n",
            "Loss: 44.887, Accuracy: 0.896, ECE: 6.597\n",
            "Loss: 68.138, Accuracy: 0.883, ECE: 11.307\n",
            "Loss: 53.718, Accuracy: 0.924, ECE: 2.938\n",
            "Loss: 72.583, Accuracy: 0.887, ECE: 9.300\n",
            "Loss: 63.518, Accuracy: 0.893, ECE: 5.307\n",
            "Loss: 65.472, Accuracy: 0.891, ECE: 2.683\n",
            "Loss: 44.663, Accuracy: 0.896, ECE: 3.994\n",
            "Loss: 23.594, Accuracy: 0.943, ECE: 2.395\n",
            "Loss: 59.393, Accuracy: 0.931, ECE: 7.927\n",
            "Loss: 48.379, Accuracy: 0.924, ECE: 3.528\n",
            "Loss: 44.442, Accuracy: 0.900, ECE: 3.638\n",
            "Loss: 66.818, Accuracy: 0.883, ECE: 17.702\n",
            "Loss: 96.660, Accuracy: 0.861, ECE: 5.944\n",
            "Loss: 33.004, Accuracy: 0.898, ECE: 33.884\n",
            "Loss: 63.066, Accuracy: 0.900, ECE: 3.906\n",
            "Loss: 53.279, Accuracy: 0.911, ECE: 38.954\n",
            "Loss: 46.697, Accuracy: 0.920, ECE: 2.990\n",
            "Loss: 62.880, Accuracy: 0.894, ECE: 4.238\n",
            "Loss: 53.267, Accuracy: 0.917, ECE: 2.599\n",
            "Loss: 46.954, Accuracy: 0.913, ECE: 8.470\n",
            "Loss: 41.224, Accuracy: 0.904, ECE: 4.547\n",
            "Loss: 36.800, Accuracy: 0.898, ECE: 71.628\n",
            "Loss: 75.858, Accuracy: 0.896, ECE: 3.334\n",
            "Loss: 81.992, Accuracy: 0.870, ECE: 4.460\n",
            "Loss: 42.907, Accuracy: 0.915, ECE: 5.195\n",
            "Loss: 38.486, Accuracy: 0.928, ECE: 1.724\n",
            "Loss: 69.327, Accuracy: 0.907, ECE: 2.958\n",
            "Loss: 61.811, Accuracy: 0.889, ECE: 2.960\n",
            "Loss: 53.019, Accuracy: 0.913, ECE: 2.949\n",
            "Loss: 73.613, Accuracy: 0.876, ECE: 20.397\n",
            "Loss: 54.956, Accuracy: 0.911, ECE: 4.343\n",
            "Loss: 67.192, Accuracy: 0.900, ECE: 4.622\n",
            "Loss: 53.359, Accuracy: 0.928, ECE: 3.774\n",
            "Loss: 42.689, Accuracy: 0.946, ECE: 10.149\n",
            "Loss: 74.513, Accuracy: 0.889, ECE: 4.456\n",
            "Loss: 49.414, Accuracy: 0.904, ECE: 6.011\n",
            "Loss: 64.075, Accuracy: 0.902, ECE: 3.745\n",
            "Loss: 46.603, Accuracy: 0.898, ECE: 2.911\n",
            "Loss: 47.735, Accuracy: 0.909, ECE: 2.148\n",
            "Loss: 40.384, Accuracy: 0.928, ECE: 6.855\n",
            "Loss: 32.738, Accuracy: 0.937, ECE: 2.262\n",
            "Loss: 35.177, Accuracy: 0.915, ECE: 25.995\n",
            "Loss: 55.918, Accuracy: 0.896, ECE: 4.313\n",
            "Loss: 37.293, Accuracy: 0.919, ECE: 3.903\n",
            "Loss: 57.176, Accuracy: 0.913, ECE: 2.346\n",
            "Loss: 60.969, Accuracy: 0.915, ECE: 5.072\n",
            "Loss: 50.265, Accuracy: 0.878, ECE: 5.850\n",
            "Loss: 83.462, Accuracy: 0.893, ECE: 12.563\n",
            "Loss: 55.755, Accuracy: 0.926, ECE: 3.078\n",
            "Loss: 40.363, Accuracy: 0.920, ECE: 4.614\n",
            "Loss: 66.190, Accuracy: 0.922, ECE: 46.807\n",
            "Loss: 25.968, Accuracy: 0.944, ECE: 2.385\n",
            "Loss: 53.993, Accuracy: 0.891, ECE: 5.585\n",
            "Loss: 51.378, Accuracy: 0.922, ECE: 4.167\n",
            "Loss: 70.294, Accuracy: 0.891, ECE: 6.242\n",
            "Loss: 68.399, Accuracy: 0.902, ECE: 7.179\n",
            "Loss: 43.197, Accuracy: 0.915, ECE: 10.613\n",
            "Loss: 72.631, Accuracy: 0.902, ECE: 6.482\n",
            "Loss: 64.653, Accuracy: 0.904, ECE: 7.300\n",
            "Loss: 66.923, Accuracy: 0.900, ECE: 6.299\n",
            "Loss: 57.628, Accuracy: 0.904, ECE: 5.893\n",
            "Loss: 48.982, Accuracy: 0.894, ECE: 14.299\n",
            "Loss: 39.443, Accuracy: 0.904, ECE: 8.327\n",
            "Loss: 49.657, Accuracy: 0.911, ECE: 5.804\n",
            "Loss: 55.669, Accuracy: 0.909, ECE: 8.536\n",
            "Loss: 37.145, Accuracy: 0.922, ECE: 2.282\n",
            "Loss: 32.878, Accuracy: 0.928, ECE: 17.881\n",
            "Loss: 60.564, Accuracy: 0.891, ECE: 2.826\n",
            "Loss: 54.819, Accuracy: 0.907, ECE: 3.205\n",
            "Loss: 34.769, Accuracy: 0.931, ECE: 3.238\n",
            "Loss: 46.508, Accuracy: 0.937, ECE: 7.423\n",
            "Loss: 63.977, Accuracy: 0.915, ECE: 19.748\n",
            "Loss: 54.705, Accuracy: 0.913, ECE: 14.383\n",
            "Loss: 63.436, Accuracy: 0.891, ECE: 10.380\n",
            "Loss: 44.270, Accuracy: 0.924, ECE: 35.037\n",
            "Loss: 30.366, Accuracy: 0.915, ECE: 12.641\n",
            "Loss: 40.608, Accuracy: 0.902, ECE: 17.455\n",
            "Loss: 44.585, Accuracy: 0.917, ECE: 11.627\n",
            "Loss: 44.487, Accuracy: 0.935, ECE: 2.017\n",
            "Loss: 52.346, Accuracy: 0.900, ECE: 3.518\n",
            "Loss: 44.464, Accuracy: 0.919, ECE: 4.426\n",
            "Loss: 32.911, Accuracy: 0.950, ECE: 3.065\n",
            "Loss: 26.978, Accuracy: 0.930, ECE: 2.183\n",
            "Loss: 47.658, Accuracy: 0.915, ECE: 5.632\n",
            "Loss: 60.861, Accuracy: 0.919, ECE: 18.521\n",
            "Loss: 58.446, Accuracy: 0.902, ECE: 30.039\n",
            "Loss: 35.285, Accuracy: 0.933, ECE: 4.068\n",
            "Loss: 77.074, Accuracy: 0.909, ECE: 5.931\n",
            "Loss: 46.830, Accuracy: 0.931, ECE: 2.520\n",
            "Loss: 57.608, Accuracy: 0.904, ECE: 8.229\n",
            "Loss: 63.003, Accuracy: 0.896, ECE: 18.250\n",
            "Loss: 48.069, Accuracy: 0.922, ECE: 2.945\n",
            "Loss: 47.211, Accuracy: 0.937, ECE: 4.138\n",
            "Loss: 66.336, Accuracy: 0.885, ECE: 5.857\n",
            "Loss: 45.276, Accuracy: 0.924, ECE: 5.438\n",
            "Loss: 70.534, Accuracy: 0.893, ECE: 8.794\n",
            "Loss: 45.974, Accuracy: 0.920, ECE: 27.079\n",
            "Loss: 76.953, Accuracy: 0.904, ECE: 3.024\n",
            "Loss: 49.199, Accuracy: 0.906, ECE: 3.913\n",
            "Loss: 50.363, Accuracy: 0.931, ECE: 5.573\n",
            "Loss: 55.488, Accuracy: 0.904, ECE: 16.811\n",
            "Loss: 27.328, Accuracy: 0.926, ECE: 6.891\n",
            "Loss: 53.424, Accuracy: 0.907, ECE: 5.164\n",
            "Loss: 71.192, Accuracy: 0.920, ECE: 6.184\n",
            "Loss: 36.660, Accuracy: 0.920, ECE: 2.241\n",
            "Loss: 71.535, Accuracy: 0.891, ECE: 4.636\n",
            "Loss: 40.611, Accuracy: 0.931, ECE: 5.796\n",
            "Loss: 39.502, Accuracy: 0.922, ECE: 6.298\n",
            "Loss: 55.013, Accuracy: 0.898, ECE: 6.337\n",
            "Loss: 56.678, Accuracy: 0.907, ECE: 5.036\n",
            "Loss: 33.211, Accuracy: 0.928, ECE: 14.322\n",
            "Loss: 36.803, Accuracy: 0.902, ECE: 5.161\n",
            "Loss: 42.974, Accuracy: 0.920, ECE: 2.588\n",
            "Loss: 42.007, Accuracy: 0.917, ECE: 2.531\n",
            "Loss: 62.507, Accuracy: 0.896, ECE: 3.763\n",
            "Loss: 42.761, Accuracy: 0.930, ECE: 2.734\n",
            "Loss: 37.946, Accuracy: 0.928, ECE: 2.553\n",
            "Loss: 81.054, Accuracy: 0.887, ECE: 6.852\n",
            "Loss: 35.711, Accuracy: 0.933, ECE: 3.530\n",
            "Loss: 41.200, Accuracy: 0.926, ECE: 3.790\n",
            "Loss: 42.069, Accuracy: 0.909, ECE: 3.526\n",
            "Loss: 30.321, Accuracy: 0.917, ECE: 3.946\n",
            "Loss: 66.192, Accuracy: 0.906, ECE: 2.424\n",
            "Loss: 82.187, Accuracy: 0.881, ECE: 27.466\n",
            "Loss: 55.613, Accuracy: 0.917, ECE: 5.932\n",
            "Loss: 55.751, Accuracy: 0.907, ECE: 6.619\n",
            "Loss: 57.537, Accuracy: 0.907, ECE: 13.115\n",
            "Loss: 53.154, Accuracy: 0.904, ECE: 2.565\n",
            "Loss: 67.154, Accuracy: 0.915, ECE: 13.327\n",
            "Loss: 36.064, Accuracy: 0.930, ECE: 2.633\n",
            "Loss: 42.557, Accuracy: 0.930, ECE: 6.099\n",
            "Loss: 53.744, Accuracy: 0.917, ECE: 7.617\n",
            "Loss: 51.835, Accuracy: 0.900, ECE: 2.958\n",
            "Loss: 60.943, Accuracy: 0.906, ECE: 3.030\n",
            "Loss: 44.857, Accuracy: 0.915, ECE: 5.627\n",
            "Loss: 56.537, Accuracy: 0.917, ECE: 2.865\n",
            "Loss: 21.606, Accuracy: 0.946, ECE: 1.753\n",
            "Loss: 39.048, Accuracy: 0.907, ECE: 5.973\n",
            "Loss: 43.425, Accuracy: 0.909, ECE: 8.404\n",
            "Loss: 55.151, Accuracy: 0.920, ECE: 3.375\n",
            "Loss: 50.377, Accuracy: 0.900, ECE: 2.668\n",
            "Loss: 45.383, Accuracy: 0.906, ECE: 11.890\n",
            "Loss: 34.703, Accuracy: 0.919, ECE: 2.911\n",
            "Loss: 52.074, Accuracy: 0.922, ECE: 2.681\n",
            "Loss: 36.944, Accuracy: 0.913, ECE: 4.819\n",
            "Loss: 41.657, Accuracy: 0.928, ECE: 3.114\n",
            "Loss: 43.138, Accuracy: 0.915, ECE: 6.166\n",
            "Loss: 67.459, Accuracy: 0.913, ECE: 3.537\n",
            "Loss: 35.101, Accuracy: 0.917, ECE: 3.669\n",
            "Loss: 30.354, Accuracy: 0.920, ECE: 2.198\n",
            "Loss: 47.105, Accuracy: 0.898, ECE: 488.166\n",
            "Loss: 41.099, Accuracy: 0.911, ECE: 24.190\n",
            "Loss: 61.678, Accuracy: 0.904, ECE: 6.403\n",
            "Loss: 54.423, Accuracy: 0.930, ECE: 2.328\n",
            "Loss: 36.379, Accuracy: 0.919, ECE: 3.132\n",
            "Loss: 50.546, Accuracy: 0.922, ECE: 4.832\n",
            "Loss: 45.954, Accuracy: 0.909, ECE: 12.509\n",
            "Loss: 49.616, Accuracy: 0.906, ECE: 10.274\n",
            "Loss: 54.201, Accuracy: 0.922, ECE: 2.423\n",
            "Loss: 38.626, Accuracy: 0.928, ECE: 2.279\n",
            "Loss: 27.171, Accuracy: 0.954, ECE: 2.213\n",
            "Loss: 31.020, Accuracy: 0.926, ECE: 1.913\n",
            "Loss: 71.703, Accuracy: 0.930, ECE: 8.179\n",
            "Loss: 62.208, Accuracy: 0.904, ECE: 2.402\n",
            "Loss: 44.267, Accuracy: 0.920, ECE: 8.017\n",
            "Loss: 32.757, Accuracy: 0.952, ECE: 6.392\n",
            "Loss: 30.848, Accuracy: 0.924, ECE: 6.007\n",
            "Loss: 34.862, Accuracy: 0.935, ECE: 2.738\n",
            "Loss: 53.228, Accuracy: 0.902, ECE: 5.076\n",
            "Loss: 31.646, Accuracy: 0.928, ECE: 2.136\n",
            "Loss: 45.681, Accuracy: 0.935, ECE: 8.644\n",
            "Loss: 45.258, Accuracy: 0.900, ECE: 3.254\n",
            "Loss: 43.140, Accuracy: 0.943, ECE: 4.968\n",
            "Loss: 53.052, Accuracy: 0.920, ECE: 7.106\n",
            "Loss: 31.211, Accuracy: 0.924, ECE: 2.907\n",
            "Loss: 57.272, Accuracy: 0.920, ECE: 10.711\n",
            "Loss: 32.476, Accuracy: 0.928, ECE: 1.970\n",
            "Loss: 66.815, Accuracy: 0.913, ECE: 28.081\n",
            "Loss: 38.007, Accuracy: 0.919, ECE: 4.284\n",
            "Loss: 60.745, Accuracy: 0.906, ECE: 5.702\n",
            "Loss: 51.117, Accuracy: 0.928, ECE: 3.293\n",
            "Loss: 38.500, Accuracy: 0.941, ECE: 2.363\n",
            "Loss: 38.913, Accuracy: 0.904, ECE: 4.120\n",
            "Loss: 47.940, Accuracy: 0.926, ECE: 3.356\n",
            "Loss: 70.031, Accuracy: 0.900, ECE: 5.685\n",
            "Loss: 64.119, Accuracy: 0.909, ECE: 3.795\n",
            "Loss: 41.109, Accuracy: 0.913, ECE: 3.123\n",
            "Loss: 50.556, Accuracy: 0.926, ECE: 5.640\n",
            "Loss: 48.388, Accuracy: 0.902, ECE: 4.009\n",
            "Loss: 40.602, Accuracy: 0.933, ECE: 1.765\n",
            "Loss: 44.750, Accuracy: 0.939, ECE: 2.383\n",
            "Loss: 32.836, Accuracy: 0.920, ECE: 2.433\n",
            "Loss: 52.280, Accuracy: 0.896, ECE: 11.428\n",
            "Loss: 38.311, Accuracy: 0.915, ECE: 11.886\n",
            "Loss: 44.458, Accuracy: 0.911, ECE: 7.392\n",
            "Loss: 31.554, Accuracy: 0.952, ECE: 2.305\n",
            "Loss: 39.210, Accuracy: 0.946, ECE: 2.323\n",
            "Loss: 42.722, Accuracy: 0.935, ECE: 2.668\n",
            "Loss: 49.571, Accuracy: 0.922, ECE: 7.575\n",
            "Loss: 59.981, Accuracy: 0.924, ECE: 5.994\n",
            "Loss: 34.043, Accuracy: 0.943, ECE: 1.823\n",
            "Loss: 38.354, Accuracy: 0.919, ECE: 3.416\n",
            "Loss: 71.534, Accuracy: 0.893, ECE: 4.534\n",
            "Loss: 49.917, Accuracy: 0.919, ECE: 4.301\n",
            "Loss: 39.279, Accuracy: 0.943, ECE: 3.884\n",
            "Loss: 45.431, Accuracy: 0.928, ECE: 2.350\n",
            "Loss: 31.808, Accuracy: 0.922, ECE: 10.470\n",
            "Loss: 40.420, Accuracy: 0.928, ECE: 5.737\n",
            "Loss: 36.863, Accuracy: 0.933, ECE: 8.312\n",
            "Loss: 27.660, Accuracy: 0.924, ECE: 9.362\n",
            "Loss: 43.285, Accuracy: 0.900, ECE: 8.978\n",
            "Loss: 42.289, Accuracy: 0.917, ECE: 13.858\n",
            "Loss: 47.128, Accuracy: 0.924, ECE: 2.701\n",
            "Loss: 54.304, Accuracy: 0.930, ECE: 3.104\n",
            "Loss: 38.174, Accuracy: 0.928, ECE: 3.386\n",
            "Loss: 29.111, Accuracy: 0.922, ECE: 2.960\n",
            "Loss: 39.146, Accuracy: 0.933, ECE: 36.926\n",
            "Loss: 68.342, Accuracy: 0.878, ECE: 64.894\n",
            "Loss: 20.175, Accuracy: 0.928, ECE: 2.443\n",
            "Loss: 32.103, Accuracy: 0.911, ECE: 8.227\n",
            "Loss: 51.518, Accuracy: 0.898, ECE: 6.654\n",
            "Loss: 39.649, Accuracy: 0.937, ECE: 9.802\n",
            "Loss: 33.055, Accuracy: 0.931, ECE: 8.485\n",
            "Loss: 38.895, Accuracy: 0.917, ECE: 2.569\n",
            "Loss: 54.274, Accuracy: 0.913, ECE: 4.515\n",
            "Loss: 43.249, Accuracy: 0.920, ECE: 2.242\n",
            "Loss: 37.506, Accuracy: 0.939, ECE: 8.125\n",
            "Loss: 60.777, Accuracy: 0.907, ECE: 3.582\n",
            "Loss: 36.495, Accuracy: 0.917, ECE: 2.216\n",
            "Loss: 20.909, Accuracy: 0.944, ECE: 135.127\n",
            "Loss: 63.853, Accuracy: 0.876, ECE: 10.736\n",
            "Loss: 24.618, Accuracy: 0.941, ECE: 5.112\n",
            "Loss: 50.745, Accuracy: 0.909, ECE: 3.286\n",
            "Loss: 48.990, Accuracy: 0.915, ECE: 3.224\n",
            "Loss: 19.493, Accuracy: 0.941, ECE: 1.815\n",
            "Loss: 28.584, Accuracy: 0.935, ECE: 1.860\n",
            "Loss: 30.504, Accuracy: 0.944, ECE: 13.281\n",
            "Loss: 65.009, Accuracy: 0.911, ECE: 4.988\n",
            "Loss: 41.966, Accuracy: 0.933, ECE: 12.849\n",
            "Loss: 64.673, Accuracy: 0.926, ECE: 37.076\n",
            "Loss: 47.419, Accuracy: 0.915, ECE: 3.955\n",
            "Loss: 35.743, Accuracy: 0.935, ECE: 1.988\n",
            "Loss: 59.741, Accuracy: 0.891, ECE: 3.307\n",
            "Loss: 21.327, Accuracy: 0.930, ECE: 2.614\n",
            "Loss: 50.902, Accuracy: 0.915, ECE: 37.459\n",
            "Loss: 30.299, Accuracy: 0.920, ECE: 2.230\n",
            "Loss: 23.839, Accuracy: 0.946, ECE: 9.963\n",
            "Loss: 80.990, Accuracy: 0.904, ECE: 9.635\n",
            "Loss: 53.923, Accuracy: 0.928, ECE: 10.234\n",
            "Loss: 38.813, Accuracy: 0.926, ECE: 2.101\n",
            "Loss: 47.732, Accuracy: 0.900, ECE: 5.651\n",
            "Loss: 20.921, Accuracy: 0.933, ECE: 5.056\n",
            "Loss: 58.247, Accuracy: 0.896, ECE: 5.562\n",
            "Loss: 44.020, Accuracy: 0.926, ECE: 4.243\n",
            "Loss: 33.480, Accuracy: 0.935, ECE: 2.075\n",
            "Loss: 53.162, Accuracy: 0.931, ECE: 7.011\n",
            "Loss: 52.069, Accuracy: 0.906, ECE: 3.993\n",
            "Loss: 44.680, Accuracy: 0.941, ECE: 5.382\n",
            "Loss: 52.885, Accuracy: 0.919, ECE: 2.483\n",
            "Loss: 47.232, Accuracy: 0.915, ECE: 3.120\n",
            "Loss: 35.418, Accuracy: 0.935, ECE: 18.840\n",
            "Loss: 37.985, Accuracy: 0.931, ECE: 1.568\n",
            "Loss: 46.351, Accuracy: 0.924, ECE: 4.206\n",
            "Loss: 62.388, Accuracy: 0.906, ECE: 9.061\n",
            "Loss: 22.874, Accuracy: 0.941, ECE: 2.565\n",
            "Loss: 39.831, Accuracy: 0.931, ECE: 3.491\n",
            "Loss: 32.546, Accuracy: 0.937, ECE: 4.247\n",
            "CPU times: user 7min 40s, sys: 14.4 s, total: 7min 54s\n",
            "Wall time: 8min 13s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GD9yFD1Njiv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "C = np.zeros(22)\n",
        "C[0] = 0 # A - Alanine - Ala -> hydrophobic side chain = 0\n",
        "C[1] = 1 # R - Arginine - Arg -> positively charged = 1\n",
        "C[2] = 2 # N - Asparagine - Asn -> polar uncharged = 2\n",
        "C[3] = 3 # D - Aspartic Acid - Asp -> negatively charged = 3\n",
        "C[4] = 4 # C - Cysteine - Cys -> special cases = 4\n",
        "C[5] = 2 # Q - Glutamine - Gln -> polar uncharged = 2\n",
        "C[6] = 3 # E - Glatamic Acid - Glu -> negatively charged = 3\n",
        "C[7] = 4 # G - Glycine - Gly -> special cases = 4\n",
        "C[8] = 1 # H - Histidine - His -> positively charged = 1\n",
        "C[9] = 0 # I - Isoleucine - Ile -> hydrophobic side chain = 0\n",
        "C[10] = 0 # L - Leucine - Leu -> hydrophobic side chain = 0\n",
        "C[11] = 1 # K - Lysine - Lys -> positively charged = 1\n",
        "C[12] = 0 # M - Methionine - Met -> hydrophobic side chain = 0\n",
        "C[13] = 0 # F - Phenylalanine - Phe -> hydrophobic side chain = 0\n",
        "C[14] = 4 # P - Proline - Pro -> special cases = 4\n",
        "C[15] = 2 # S - Serine - Ser -> polar uncharged = 2\n",
        "C[16] = 2 # T - Threonine - Thr -> polar uncharged = 2\n",
        "C[17] = 0 # W - Tryptophan - Trp -> hydrophobic side chain = 0\n",
        "C[18] = 0 #Y - Tyrosine - Tyr -> hydrophobic side chain = 0\n",
        "C[19] = 0 #V - Valine - Val -> hydrophobic side chain = 0\n",
        "C[20] = 5 #- gaps\n",
        "C[21] = 5 # * masks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIW0a7P0_m5W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "W = final_layer.weights[0].numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EULEmMM-APNK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "W_embedded = umap.UMAP(\n",
        "    n_neighbors=3,\n",
        "    min_dist=0.05,\n",
        "    n_components=2,\n",
        "    random_state=0,\n",
        ").fit_transform(W.T)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDPPCDf4BzI9",
        "colab_type": "code",
        "outputId": "9c6d1531-c20f-487f-ffa7-919d6178d28e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 594
        }
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "ax.scatter(W_embedded[:,0], W_embedded[:,1], c=C, s=75.0*np.ones(22), cmap='rainbow')\n",
        "ax.grid(None)\n",
        "\n",
        "\n",
        "k=0\n",
        "for i in \"ARNDCQEGHILKMFPSTWYV-*\":\n",
        "  ax.annotate(i, (W_embedded[k,0], W_embedded[k,1]+0.1))\n",
        "  k += 1"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlYAAAJBCAYAAACEdvs8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVisDsbk7zT0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}